{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "This notebook contains the implementation of several loss functions in Python. The functions are written inefficiently via a for loop for understanding and vectorized for actual use. The two versions are then verified to be equivalent and compared in running time. Finally, the numerical and calculated gradients are compared.\n",
    "\n",
    "The four loss functions are:\n",
    "* Logistic Loss\n",
    "* Hinge Loss/SVMs\n",
    "* Simple Two Layer Function\n",
    "* Least Squares Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# n = number of instances\n",
    "n = 100 # Change to 10000 to verify scalability\n",
    "\n",
    "# m = number of features\n",
    "m = 1000 # Change to 1000 to verify stability\n",
    "\n",
    "# X = random dataset\n",
    "X = [[random.randint(-100, 100) + random.random() for i in range(m)] for j in range(n)]\n",
    "\n",
    "# y = random n-dimensional vector\n",
    "y = [-1 if(random.randint(0,1)==0) else 1 for i in range(n)]\n",
    "y_cont = [random.random() for i in range(n)]\n",
    "\n",
    "# w = random m-dimensional vector\n",
    "w = [random.randint(-1, 1) + random.random() for i in range(m)]\n",
    "\n",
    "# Define the numerical gradient function:\n",
    "def numericalGrad(funObj, w, epsilon):\n",
    "    m = len(w)\n",
    "    grad = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        wp = np.copy(w)\n",
    "        wn = np.copy(w)\n",
    "        wp[i] = w[i] + epsilon\n",
    "        wn[i] = w[i] - epsilon\n",
    "        grad[i] = (funObj(wp) - funObj(wn)) / (2 * epsilon)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop and vectorized loss function comparisons:\n",
      "    For loop version took 0.07178616523742676 time.\n",
      "    Vectorized version took 0.015626907348632812 time.\n",
      "    Loss functions are very close to equal. Difference= 1.4551915228366852e-11\n",
      "    Gradient functions are very close to equal. Max difference= 2.2737367544323206e-13\n",
      "\n",
      "\n",
      "Numerical and calculated gradient comparisons:\n",
      "    Largest difference between gradients = 3.9140019012506855e-06\n",
      "    Smallest difference between gradients = 8.060823120104033e-10\n",
      "    Average difference between gradients = 9.06342876058952e-07\n",
      "    Median difference between gradients = 7.408490390048428e-07\n"
     ]
    }
   ],
   "source": [
    "# Logistic Loss\n",
    "\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "# For loop based loss function:\n",
    "def LogisticLossFun_ForLoop(w, X, y, lam):\n",
    "    f = lam * sum(w[i] ** 2 for i in range(m)) + sum([np.log1p(np.exp(-y[i] * np.dot(w, X[i])))\n",
    "             if y[i] * np.dot(w, X[i]) > 0\n",
    "             else np.log1p(np.exp(y[i] * np.dot(w, X[i]))) - (y[i] * np.dot(w, X[i]))\n",
    "             for i in range(n)])\n",
    "    \n",
    "    g = [2 * lam * w[i] for i in range(m)] + sum([np.array((-y[i] / (1 + np.exp(y[i] * np.dot(w, X[i])))) * np.array(X[i]))\n",
    "             if not np.isinf(np.exp(y[i] * np.dot(w, X[i])))\n",
    "             else np.zeros(m)\n",
    "             for i in range(n)])\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Vectorized version of loss function:\n",
    "def LogisticLossFun(w, X, y, lam):\n",
    "    XTw = np.dot(X, w)\n",
    "    yXTw = np.multiply(y, XTw)\n",
    "    \n",
    "    pos = yXTw > 0\n",
    "    neg = np.invert(pos)\n",
    "    expYXTw = np.exp(yXTw)\n",
    "    \n",
    "    logexp = np.zeros(yXTw.shape)\n",
    "    logexp[pos] = np.log1p(np.exp(-1 * yXTw[pos]))\n",
    "    logexp[neg] = np.log1p(expYXTw[neg]) - yXTw[neg]\n",
    "    \n",
    "    f = lam * sum(np.square(w)) + sum(logexp)\n",
    "    g = np.multiply(2 * lam, w) + np.dot(np.divide(-1 * np.array(y), 1 + expYXTw), X)\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Made sure it is numerically stable and scalable -> Change m and n above\n",
    "\n",
    "\n",
    "# Compare the for loop based loss function with the vectorized version:\n",
    "t0 = time.time()\n",
    "forLoop = LogisticLossFun_ForLoop(w, X, y, 1)\n",
    "t1 = time.time()\n",
    "vect = LogisticLossFun(w, X, y, 1)\n",
    "t2 = time.time()\n",
    "\n",
    "fDiff = abs(forLoop[0] - vect[0])\n",
    "gMaxDiff = max(abs(forLoop[1] - vect[1]))\n",
    "\n",
    "#    Verify that the for loop based and vectorized loss functions produce the same results:\n",
    "print(\"For loop and vectorized loss function comparisons:\")\n",
    "print(\"    For loop version took\", t1-t0, \"time.\")\n",
    "print(\"    Vectorized version took\", t2-t1, \"time.\")\n",
    "\n",
    "if fDiff == 0: print(\"    Loss functions are equal.\")\n",
    "elif fDiff < 1e-5: print(\"    Loss functions are very close to equal. Difference=\", fDiff)\n",
    "else: print(\"    Loss functions are NOT equal. Difference=\", fDiff)\n",
    "    \n",
    "if gMaxDiff == 0: print(\"    Gradient functions are equal.\")\n",
    "elif gMaxDiff < 1e-5: print(\"    Gradient functions are very close to equal. Max difference=\", gMaxDiff)\n",
    "else: print(\"    Gradient functions are NOT equal. Max difference=\", gMaxDiff)\n",
    "\n",
    "\n",
    "# Numerically compute the gradient:\n",
    "#    Define the function object:\n",
    "funObj = lambda w: LogisticLossFun(w, X, y, 1)[0]\n",
    "\n",
    "#    Verify that numerical and calculated gradients are the same (or very close):\n",
    "diff = abs(numericalGrad(funObj, w, 0.00001) - vect[1])\n",
    "print(\"\\n\\nNumerical and calculated gradient comparisons:\")\n",
    "print(\"    Largest difference between gradients =\", max(diff))\n",
    "print(\"    Smallest difference between gradients =\", min(diff))\n",
    "print(\"    Average difference between gradients =\", stats.mean(diff))\n",
    "print(\"    Median difference between gradients =\", stats.median(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop and vectorized loss function comparisons:\n",
      "    For loop version took 0.035523414611816406 time.\n",
      "    Vectorized version took 0.02402472496032715 time.\n",
      "    Loss functions are very close to equal. Difference= 1.4551915228366852e-11\n",
      "    Gradient functions are equal.\n",
      "\n",
      "\n",
      "Numerical and calculated gradient comparisons:\n",
      "    Largest difference between gradients = 3.871345029438089e-06\n",
      "    Smallest difference between gradients = 4.062201242049923e-09\n",
      "    Average difference between gradients = 8.916976538790936e-07\n",
      "    Median difference between gradients = 7.306956746333526e-07\n"
     ]
    }
   ],
   "source": [
    "# Hinge Loss/SVMs\n",
    "\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "# For loop based loss function:\n",
    "def HingeLossFun_ForLoop(w, X, y, lam):\n",
    "    f = lam * sum(w[i] ** 2 for i in range(m)) + sum([max(0, 1 - y[i] * np.dot(w, X[i]))\n",
    "             for i in range(n)])\n",
    "    \n",
    "    g = [2 * lam * w[i] for i in range(m)] + sum([np.array(-y[i] * np.array(X[i]))\n",
    "             if y[i] * np.dot(w, X[i]) < 1 \n",
    "             else np.zeros(m) \n",
    "             for i in range(n)])\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Vectorized version of loss function:\n",
    "def HingeLossFun(w, X, y, lam):\n",
    "    XTw = np.dot(X, w)\n",
    "    yXTw = np.multiply(y, XTw)\n",
    "    \n",
    "    y0 = np.copy(y)\n",
    "    y0[yXTw >= 1] = 0\n",
    "    \n",
    "    f = lam * sum(np.square(w)) + sum(np.maximum(1 - yXTw, 0))\n",
    "    g = np.multiply(2 * lam, w) + np.dot(-1 * y0, X)\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Made sure it is numerically stable and scalable -> Change m and n above\n",
    "\n",
    "\n",
    "# Compare the for loop based loss function with the vectorized version:\n",
    "t0 = time.time()\n",
    "forLoop = HingeLossFun_ForLoop(w, X, y, 1)\n",
    "t1 = time.time()\n",
    "vect = HingeLossFun(w, X, y, 1)\n",
    "t2 = time.time()\n",
    "\n",
    "fDiff = abs(forLoop[0] - vect[0])\n",
    "gMaxDiff = max(abs(forLoop[1] - vect[1]))\n",
    "\n",
    "#    Verify that the for loop based and vectorized loss functions produce the same results:\n",
    "print(\"For loop and vectorized loss function comparisons:\")\n",
    "print(\"    For loop version took\", t1-t0, \"time.\")\n",
    "print(\"    Vectorized version took\", t2-t1, \"time.\")\n",
    "\n",
    "if fDiff == 0: print(\"    Loss functions are equal.\")\n",
    "elif fDiff < 1e-5: print(\"    Loss functions are very close to equal. Difference=\", fDiff)\n",
    "else: print(\"    Loss functions are NOT equal. Difference=\", fDiff)\n",
    "    \n",
    "if gMaxDiff == 0: print(\"    Gradient functions are equal.\")\n",
    "elif gMaxDiff < 1e-5: print(\"    Gradient functions are very close to equal. Max difference=\", gMaxDiff)\n",
    "else: print(\"    Gradient functions are NOT equal. Max difference=\", gMaxDiff)\n",
    "\n",
    "\n",
    "# Numerically compute the gradient:\n",
    "#    Define the function object:\n",
    "funObj = lambda w: HingeLossFun(w, X, y, 1)[0]\n",
    "\n",
    "#    Verify that numerical and calculated gradients are the same (or very close):\n",
    "diff = abs(numericalGrad(funObj, w, 0.00001) - vect[1])\n",
    "print(\"\\n\\nNumerical and calculated gradient comparisons:\")\n",
    "print(\"    Largest difference between gradients =\", max(diff))\n",
    "print(\"    Smallest difference between gradients =\", min(diff))\n",
    "print(\"    Average difference between gradients =\", stats.mean(diff))\n",
    "print(\"    Median difference between gradients =\", stats.median(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop and vectorized loss function comparisons:\n",
      "    For loop version took 0.047086477279663086 time.\n",
      "    Vectorized version took 0.024002790451049805 time.\n",
      "    Loss functions are very close to equal. Difference= 5.960464477539063e-08\n",
      "    Gradient functions are very close to equal. Max difference= 4.423782229423523e-09\n",
      "\n",
      "\n",
      "Numerical and calculated gradient comparisons:\n",
      "    Largest difference between gradients = 0.0015259151114150882\n",
      "    Smallest difference between gradients = 4.791654646396637e-07\n",
      "    Average difference between gradients = 0.00036161213599888244\n",
      "    Median difference between gradients = 0.00030101288575679064\n"
     ]
    }
   ],
   "source": [
    "# Simple Two Layer Function\n",
    "\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "# For loop based loss function:\n",
    "def Simple2LayerLossFun_ForLoop(w, X, y, lam):\n",
    "    f = lam * sum(w[i] ** 2 for i in range(m)) + sum([(y[i] - max(0, np.dot(w, X[i]))) ** 2\n",
    "             for i in range(n)])\n",
    "    \n",
    "    g = [2 * lam * w[i] for i in range(m)] + sum([np.array((-2 * y[i] + 2 * np.dot(w, X[i]))\n",
    "                      * np.array(X[i]))\n",
    "             if np.dot(w, X[i]) > 0\n",
    "             else np.zeros(m)\n",
    "             for i in range(n)])\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Vectorized version of loss function:\n",
    "def Simple2LayerLossFun(w, X, y, lam):\n",
    "    XTw = np.dot(X, w)\n",
    "    num = (-2 * np.array(y)) + (2 * XTw)\n",
    "    num[XTw <= 0] = 0\n",
    "    \n",
    "    f = lam * sum(np.square(w)) + sum(np.square(np.array(y) - np.maximum(0, XTw)))\n",
    "    g = np.multiply(2 * lam, w) + np.dot(num, X)\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Made sure it is numerically stable and scalable -> Change m and n above\n",
    "\n",
    "\n",
    "# Compare the for loop based loss function with the vectorized version:\n",
    "t0 = time.time()\n",
    "forLoop = Simple2LayerLossFun_ForLoop(w, X, y, 1)\n",
    "t1 = time.time()\n",
    "vect = Simple2LayerLossFun(w, X, y, 1)\n",
    "t2 = time.time()\n",
    "\n",
    "fDiff = abs(forLoop[0] - vect[0])\n",
    "gMaxDiff = max(abs(forLoop[1] - vect[1]))\n",
    "\n",
    "#    Verify that the for loop based and vectorized loss functions produce the same results:\n",
    "print(\"For loop and vectorized loss function comparisons:\")\n",
    "print(\"    For loop version took\", t1-t0, \"time.\")\n",
    "print(\"    Vectorized version took\", t2-t1, \"time.\")\n",
    "\n",
    "if fDiff == 0: print(\"    Loss functions are equal.\")\n",
    "elif fDiff < 1e-5: print(\"    Loss functions are very close to equal. Difference=\", fDiff)\n",
    "else: print(\"    Loss functions are NOT equal. Difference=\", fDiff)\n",
    "    \n",
    "if gMaxDiff == 0: print(\"    Gradient functions are equal.\")\n",
    "elif gMaxDiff < 1e-5: print(\"    Gradient functions are very close to equal. Max difference=\", gMaxDiff)\n",
    "else: print(\"    Gradient functions are NOT equal. Max difference=\", gMaxDiff)\n",
    "\n",
    "\n",
    "# Numerically compute the gradient:\n",
    "#    Define the function object:\n",
    "funObj = lambda w: Simple2LayerLossFun(w, X, y, 1)[0]\n",
    "\n",
    "#    Verify that numerical and calculated gradients are the same (or very close):\n",
    "diff = abs(numericalGrad(funObj, w, 0.0001) - vect[1])\n",
    "print(\"\\n\\nNumerical and calculated gradient comparisons:\")\n",
    "print(\"    Largest difference between gradients =\", max(diff))\n",
    "print(\"    Smallest difference between gradients =\", min(diff))\n",
    "print(\"    Average difference between gradients =\", stats.mean(diff))\n",
    "print(\"    Median difference between gradients =\", stats.median(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop and vectorized loss function comparisons:\n",
      "    For loop version took 0.03876328468322754 time.\n",
      "    Vectorized version took 0.016489267349243164 time.\n",
      "    Loss functions are very close to equal. Difference= 2.384185791015625e-07\n",
      "    Gradient functions are very close to equal. Max difference= 7.450580596923828e-09\n",
      "\n",
      "\n",
      "Numerical and calculated gradient comparisons:\n",
      "    Largest difference between gradients = 2.982094883918762e-06\n",
      "    Smallest difference between gradients = 1.1641532182693481e-09\n",
      "    Average difference between gradients = 6.7574422701e-07\n",
      "    Median difference between gradients = 5.860347300767899e-07\n"
     ]
    }
   ],
   "source": [
    "# Least Squares Loss\n",
    "\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import time\n",
    "\n",
    "# For loop based loss function:\n",
    "def LeastSquaresLossFun_ForLoop(w, X, y, lam):\n",
    "    f = lam * sum(w[i] ** 2 for i in range(m)) + sum([(y[i] - np.dot(w, X[i])) ** 2\n",
    "             for i in range(n)])\n",
    "    \n",
    "    g = [2 * lam * w[i] for i in range(m)] + sum([np.array((-2 * y[i] + 2 * np.dot(w, X[i])) * np.array(X[i]))\n",
    "             for i in range(n)])\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Vectorized version of loss function:\n",
    "def LeastSquaresLossFun(w, X, y, lam):\n",
    "    XTw = np.dot(X, w)\n",
    "    num = (-2 * np.array(y)) + (2 * XTw)\n",
    "    \n",
    "    f = lam * sum(np.square(w)) + sum(np.square(np.array(y) - XTw))\n",
    "    g = np.multiply(2 * lam, w) + np.dot(num, X)\n",
    "    \n",
    "    return [f, g]\n",
    "\n",
    "# Made sure it is numerically stable and scalable -> Change m and n above\n",
    "\n",
    "\n",
    "# Compare the for loop based loss function with the vectorized version:\n",
    "t0 = time.time()\n",
    "forLoop = LeastSquaresLossFun_ForLoop(w, X, y_cont, 1)\n",
    "t1 = time.time()\n",
    "vect = LeastSquaresLossFun(w, X, y_cont, 1)\n",
    "t2 = time.time()\n",
    "\n",
    "fDiff = abs(forLoop[0] - vect[0])\n",
    "gMaxDiff = max(abs(forLoop[1] - vect[1]))\n",
    "\n",
    "#    Verify that the for loop based and vectorized loss functions produce the same results:\n",
    "print(\"For loop and vectorized loss function comparisons:\")\n",
    "print(\"    For loop version took\", t1-t0, \"time.\")\n",
    "print(\"    Vectorized version took\", t2-t1, \"time.\")\n",
    "\n",
    "if fDiff == 0: print(\"    Loss functions are equal.\")\n",
    "elif fDiff < 1e-5: print(\"    Loss functions are very close to equal. Difference=\", fDiff)\n",
    "else: print(\"    Loss functions are NOT equal. Difference=\", fDiff)\n",
    "    \n",
    "if gMaxDiff == 0: print(\"    Gradient functions are equal.\")\n",
    "elif gMaxDiff < 1e-5: print(\"    Gradient functions are very close to equal. Max difference=\", gMaxDiff)\n",
    "else: print(\"    Gradient functions are NOT equal. Max difference=\", gMaxDiff)\n",
    "\n",
    "\n",
    "# Numerically compute the gradient:\n",
    "#    Define the function object:\n",
    "funObj = lambda w: LeastSquaresLossFun(w, X, y_cont, 1)[0]\n",
    "\n",
    "#    Verify that numerical and calculated gradients are the same (or very close):\n",
    "diff = abs(numericalGrad(funObj, w, 0.1) - vect[1])\n",
    "print(\"\\n\\nNumerical and calculated gradient comparisons:\")\n",
    "print(\"    Largest difference between gradients =\", max(diff))\n",
    "print(\"    Smallest difference between gradients =\", min(diff))\n",
    "print(\"    Average difference between gradients =\", stats.mean(diff))\n",
    "print(\"    Median difference between gradients =\", stats.median(diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
